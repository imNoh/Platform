{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2daf60a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkConf, SparkContext\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "806cced7",
   "metadata": {},
   "source": [
    "**SparkConf**\n",
    "사용자가 재정의해서 쓸 수 있는 설정 옵션들에 대한 키와 값을 갖고있는 객체\n",
    "\n",
    "\n",
    "**SparkContext**\n",
    "Spark 클러스터와 연결시켜주는 객체\n",
    "\n",
    "\n",
    "* Spark 모든 기능에 접근할 수 있는 시작점\n",
    "* Spark는 분산환경에서 동작하기 때문에 Driver Program 을 구동시키기 위해 SparkContext가 필요\n",
    "* SparkContext는 프로그램당 하나만 만들 수 있고 사용후에는 종료\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a7753cb",
   "metadata": {},
   "source": [
    "## SparkContext 초기화\n",
    "\n",
    "![sparkcontext](assets/sparkcontext.png)\n",
    "\n",
    "* SparkContext 객체는 내부에 자바로 동작하는 Py4J의 SparkContext와 연결\n",
    "* 이 덕분에 파이썬으로 코딩하면서도 자바 위에서 동작하는 프로그램을 작성할 수 있다 \n",
    "* RDD를 만들 수 있다"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acaae61e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure Spark \n",
    "conf = SparkConf().setMaster(\"local\").setAppName(\"uber-date-trips\")\n",
    "sc = SparkContext(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0548e00c",
   "metadata": {},
   "source": [
    "setMaster(\"local\") - 분산된 환경이 아닌 개발용 로컬 환경을 쓴다\n",
    "\n",
    "setAppName - Spark UI에서 확인 가능한 스파크 앱 이름"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e183d6b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dataset sample\n",
    "filename = \"fhvhv_tripdata_2020-03_short.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ee82272",
   "metadata": {},
   "source": [
    "## 데이터 로딩후 RDD 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7772a2dd",
   "metadata": {},
   "outputs": [
    {
     "ename": "Py4JJavaError",
     "evalue": "An error occurred while calling o22.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/NohJi/Desktop/플랫폼/data/fhvhv_tripdata_2020-03_short.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPy4JJavaError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m lines \u001b[38;5;241m=\u001b[39m sc\u001b[38;5;241m.\u001b[39mtextFile(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m./data/\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m filename)   \u001b[38;5;66;03m# .csv -> RDD object\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m header \u001b[38;5;241m=\u001b[39m \u001b[43mlines\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfirst\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mC:\\Dev\\Anaconda\\lib\\site-packages\\pyspark\\rdd.py:1903\u001b[0m, in \u001b[0;36mRDD.first\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfirst\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRDD[T]\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m T:\n\u001b[0;32m   1891\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1892\u001b[0m \u001b[38;5;124;03m    Return the first element in this RDD.\u001b[39;00m\n\u001b[0;32m   1893\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1901\u001b[0m \u001b[38;5;124;03m    ValueError: RDD is empty\u001b[39;00m\n\u001b[0;32m   1902\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1903\u001b[0m     rs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtake\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1904\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m rs:\n\u001b[0;32m   1905\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m rs[\u001b[38;5;241m0\u001b[39m]\n",
      "File \u001b[1;32mC:\\Dev\\Anaconda\\lib\\site-packages\\pyspark\\rdd.py:1850\u001b[0m, in \u001b[0;36mRDD.take\u001b[1;34m(self, num)\u001b[0m\n\u001b[0;32m   1826\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1827\u001b[0m \u001b[38;5;124;03mTake the first num elements of the RDD.\u001b[39;00m\n\u001b[0;32m   1828\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1847\u001b[0m \u001b[38;5;124;03m[91, 92, 93]\u001b[39;00m\n\u001b[0;32m   1848\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1849\u001b[0m items: List[T] \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m-> 1850\u001b[0m totalParts \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetNumPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1851\u001b[0m partsScanned \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m   1853\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(items) \u001b[38;5;241m<\u001b[39m num \u001b[38;5;129;01mand\u001b[39;00m partsScanned \u001b[38;5;241m<\u001b[39m totalParts:\n\u001b[0;32m   1854\u001b[0m     \u001b[38;5;66;03m# The number of partitions to try in this iteration.\u001b[39;00m\n\u001b[0;32m   1855\u001b[0m     \u001b[38;5;66;03m# It is ok for this number to be greater than totalParts because\u001b[39;00m\n\u001b[0;32m   1856\u001b[0m     \u001b[38;5;66;03m# we actually cap it at totalParts in runJob.\u001b[39;00m\n",
      "File \u001b[1;32mC:\\Dev\\Anaconda\\lib\\site-packages\\pyspark\\rdd.py:599\u001b[0m, in \u001b[0;36mRDD.getNumPartitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    589\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgetNumPartitions\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    590\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    591\u001b[0m \u001b[38;5;124;03m    Returns the number of partitions in RDD\u001b[39;00m\n\u001b[0;32m    592\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    597\u001b[0m \u001b[38;5;124;03m    2\u001b[39;00m\n\u001b[0;32m    598\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 599\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jrdd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39msize()\n",
      "File \u001b[1;32mC:\\Dev\\Anaconda\\lib\\site-packages\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32mC:\\Dev\\Anaconda\\lib\\site-packages\\py4j\\protocol.py:326\u001b[0m, in \u001b[0;36mget_return_value\u001b[1;34m(answer, gateway_client, target_id, name)\u001b[0m\n\u001b[0;32m    324\u001b[0m value \u001b[38;5;241m=\u001b[39m OUTPUT_CONVERTER[\u001b[38;5;28mtype\u001b[39m](answer[\u001b[38;5;241m2\u001b[39m:], gateway_client)\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m answer[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m==\u001b[39m REFERENCE_TYPE:\n\u001b[1;32m--> 326\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JJavaError(\n\u001b[0;32m    327\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name), value)\n\u001b[0;32m    329\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    330\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m Py4JError(\n\u001b[0;32m    331\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAn error occurred while calling \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m. Trace:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{3}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39m\n\u001b[0;32m    332\u001b[0m         \u001b[38;5;28mformat\u001b[39m(target_id, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m, name, value))\n",
      "\u001b[1;31mPy4JJavaError\u001b[0m: An error occurred while calling o22.partitions.\n: org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: file:/C:/Users/NohJi/Desktop/플랫폼/data/fhvhv_tripdata_2020-03_short.csv\r\n\tat org.apache.hadoop.mapred.FileInputFormat.singleThreadedListStatus(FileInputFormat.java:287)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:229)\r\n\tat org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:315)\r\n\tat org.apache.spark.rdd.HadoopRDD.getPartitions(HadoopRDD.scala:205)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.rdd.MapPartitionsRDD.getPartitions(MapPartitionsRDD.scala:49)\r\n\tat org.apache.spark.rdd.RDD.$anonfun$partitions$2(RDD.scala:300)\r\n\tat scala.Option.getOrElse(Option.scala:189)\r\n\tat org.apache.spark.rdd.RDD.partitions(RDD.scala:296)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.JavaRDDLike.partitions$(JavaRDDLike.scala:61)\r\n\tat org.apache.spark.api.java.AbstractJavaRDDLike.partitions(JavaRDDLike.scala:45)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\r\n\tat java.base/jdk.internal.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\r\n\tat java.base/jdk.internal.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\r\n\tat java.base/java.lang.reflect.Method.invoke(Method.java:566)\r\n\tat py4j.reflection.MethodInvoker.invoke(MethodInvoker.java:244)\r\n\tat py4j.reflection.ReflectionEngine.invoke(ReflectionEngine.java:357)\r\n\tat py4j.Gateway.invoke(Gateway.java:282)\r\n\tat py4j.commands.AbstractCommand.invokeMethod(AbstractCommand.java:132)\r\n\tat py4j.commands.CallCommand.execute(CallCommand.java:79)\r\n\tat py4j.ClientServerConnection.waitForCommands(ClientServerConnection.java:182)\r\n\tat py4j.ClientServerConnection.run(ClientServerConnection.java:106)\r\n\tat java.base/java.lang.Thread.run(Thread.java:834)\r\n"
     ]
    }
   ],
   "source": [
    "lines = sc.textFile(\"./data/\" + filename)   # .csv -> RDD object\n",
    "header = lines.first() "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f277156a",
   "metadata": {},
   "source": [
    "*데이터*\n",
    "```\n",
    "hvfhs_license_num,dispatching_base_num,pickup_datetime,dropoff_datetime,PULocationID,DOLocationID,SR_Flag\n",
    "HV0005,B02510,2020-03-01 00:03:40,2020-03-01 00:23:39,81,159,\n",
    "HV0005,B02510,2020-03-01 00:28:05,2020-03-01 00:38:57,168,119,\n",
    "HV0003,B02764,2020-03-01 00:03:07,2020-03-01 00:15:04,137,209,1\n",
    "HV0003,B02764,2020-03-01 00:18:42,2020-03-01 00:38:42,209,80,\n",
    "HV0003,B02764,2020-03-01 00:44:24,2020-03-01 00:58:44,256,226,\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e5e7ad9",
   "metadata": {},
   "source": [
    "## 필요한 부분만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2997f8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_lines = lines.filter(lambda row:row != header) # all lines excepting the header"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00690052",
   "metadata": {},
   "source": [
    "아래와 같습니다\n",
    "```python\n",
    "def f(row):\n",
    "    return row != header\n",
    "lines.filter(f) \n",
    "```\n",
    "\n",
    "*데이터*\n",
    "```\n",
    "HV0005,B02510,2020-03-01 00:03:40,2020-03-01 00:23:39,81,159, \n",
    "HV0005,B02510,2020-03-01 00:28:05,2020-03-01 00:38:57,168,119,\n",
    "HV0003,B02764,2020-03-01 00:03:07,2020-03-01 00:15:04,137,209,1\n",
    "HV0003,B02764,2020-03-01 00:18:42,2020-03-01 00:38:42,209,80,\n",
    "HV0003,B02764,2020-03-01 00:44:24,2020-03-01 00:58:44,256,226,\n",
    "...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d6d5681a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates = filtered_lines.map(lambda x: x.split(\",\")[2].split(\" \")[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0be738a",
   "metadata": {},
   "source": [
    "**map()**함수로 우리가 원하는 부분만 추출 할 수 있다\n",
    "\n",
    "\n",
    "추출하는 함수\n",
    "```python\n",
    "lambda x: x.split(\",\")[2].split(\" \")[0]\n",
    "\n",
    "```\n",
    "아래와 같다\n",
    "\n",
    "```python]\n",
    "def f(x):\n",
    "    return x.split(\",\")[2].split(\" \")[0]\n",
    "```\n",
    "\n",
    "\n",
    "오리지널 데이터\n",
    "```\n",
    "HV0005,B02510,2020-03-01 00:03:40,2020-03-01 00:23:39,81,159,\n",
    "```\n",
    "\n",
    "x.split(\",\")\n",
    "```\n",
    "[HV0005,B02510,2020-03-01 00:03:40,2020-03-01 00:23:39,81,159,]\n",
    "```\n",
    "\n",
    "\n",
    "x.split(\",\")[2]\n",
    "```\n",
    "[2020-03-01 00:03:40]\n",
    "```\n",
    "\n",
    "\n",
    "x.split(\",\")[2].split(\" \")\n",
    "```\n",
    "[2020-03-01,00:03:40]\n",
    "```\n",
    "\n",
    "x.split(\",\")[2].split(\" \")[0]\n",
    "```\n",
    "2020-03-01\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118b51e7",
   "metadata": {},
   "source": [
    "## CountByValue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "befafcb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "result = dates.countByValue() # Transformation, Action"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91841c20",
   "metadata": {},
   "source": [
    "값이 얼마나 등장하는지 세준다 \n",
    "\n",
    "예)\n",
    "```\n",
    "2020-03-01\n",
    "2020-03-01\n",
    "2020-03-01\n",
    "2020-03-02\n",
    "2020-03-02\n",
    "2020-03-03\n",
    "```\n",
    "countByValue()\n",
    "```\n",
    "(2020-03-01,3)\n",
    "(2020-03-02,2)\n",
    "(2020-03-03,1)\n",
    "```\n",
    "\n",
    "\n",
    "**result는 이제 더이상 RDD가 아닌 Python 객체**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac0de56a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results as a csv file \n",
    "pd.Series(result, name=\"trips\").to_csv(\"trips_date.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe8f9c44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline  \n",
    "\n",
    "trips = pd.read_csv(\"trips_date.csv\")\n",
    "trips.plot()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
